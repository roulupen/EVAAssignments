### Architectural Basics:
- ##### How many layers,
   - Number of layers in a network depends on receptive fields, depends on size of the kernel, and available computation units. If we have less number of computation units then we can reduce number of layers using Maxpooling layer(or transition layer).
- ##### MaxPooling,
    - MaxPooling is used to reduce number of kernels(computational unit requirment) and Maxpooling takes values with higher weightage to output, so it reduces any noise but at the same time it may supress some feature as well.
    - [Image which describes Maxpooling]
- ##### 1x1 Convolutions,
    - 1x1 convolution is used to reduce number of channel in a network. Before invention of 1x1 convolution we were using normal convolution for reducing number of channels.
    - Example: 392x392x256 | (3x3x256)x512 | 390x390x512
    - MaxPooling
    - 195x195x512 | (?x?x512)x32    | ?x?x32 RF of 22x22
    - By reducing the number of kernels from 512 to 32 will work fine but here we are compting 512 to get 32 this is not correct intuitively.
    - The **1x1** convultion doesn't revalues all 512 kernels to pick 32, instead it merges all 512 to generate 32. If some feature will have less weitage it will be supressed in output and with back-propagation it helps the network to reevalutes those weights to give better prediction.
- ##### 3x3 Convolutions,
    - We can use any other size kernels like 5x5, 7x7, 6x10. But to capture all the pixels of an image in receptive fields 3x3 channels uses less coputational power. Using 3x3 we can acheive all kinds of kernel.
- ##### Receptive Field,
    - In a network before applying any transitional layer we should be sure that the size of the receptive field at that layer is equal to object size in that images or else the network may not identify all the features in the image.
- ##### SoftMax,
    - Softmax activation function is used for multi class classification problem. Using softmax each output class is associated with a probalily like score and based on that model predict what would be the output of a given input. Sum of all softmax score for different classes of a given input will be 1.
- ##### Learning Rate,
    - Learning rate is a hyper tuning paramter, it decides how fast we can arrive at the solution. This paramter used in grdient ascent/descent algorithm to decide the step size. The higher the learning rate faster the learning but when we approach towards the final solution if the learning rate is more then we may not reach the ideal solution and if we are keeping learning rate lower then we have to run the model training for a longer period of time. So, in-order to come for a mutual solution initial while starting of the model training the learning rate can be higher and after some cut-off the learning rate can reduce.
- ##### Kernels and how do we decide the number of kernels?
    - Genrally in a network the number of kernels placed in each layer in incremental fashion, like 8, 16, 32, 64... The kernel size directly affects the number of paramter, so higher the kernel size more coputation power needed to train the model. If we have less coputation power then we need to reduce number of kernels and also Maxpooling is used to reduce number of kernels in a network.
- ##### Batch Normalization,
    - 
- ##### Image Normalization,
    -    
- ##### Position of MaxPooling,
    - 
- ##### Concept of Transition Layers,
    - 
- ##### Position of Transition Layer,
    - 
- ##### Number of Epochs and when to increase them,
    - 
- ##### DropOut
    - 
- ##### When do we introduce DropOut, or when do we know we have some overfitting
    - 
- ##### The distance of MaxPooling from Prediction,
    - 
- ##### The distance of Batch Normalization from Prediction,
    - 
- ##### When do we stop convolutions and go ahead with a larger kernel or some other alternative (which we have not yet covered)
    - 
- ##### How do we know our network is not going well, comparatively, very early
    - 
- ##### Batch Size, and effects of batch size
    - 
- ##### When to add validation checks
    - 
- ##### LR schedule and concept behind it
    - 
- ##### Adam vs SGD
    -    
