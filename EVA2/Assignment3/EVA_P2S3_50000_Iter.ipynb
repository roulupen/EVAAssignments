{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EVA P2S3 - 50000 Iter.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roulupen/EVAAssignments/blob/master/EVA2/Assignment3/EVA_P2S3_50000_Iter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jofyc9OC4Qcf",
        "colab_type": "text"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahBVnrNc3E0U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "plt.style.use('seaborn-white')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHBHiGL2NBXc",
        "colab_type": "code",
        "outputId": "f7cc3b05-323b-4451-9516-f87c7850827a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!git clone https://github.com/roulupen/EVAAssignments.git"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'EVAAssignments' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crQSAaIz4SkA",
        "colab_type": "text"
      },
      "source": [
        "# Read and process data. \n",
        "\n",
        "Download the file from this URL: https://drive.google.com/file/d/1UWWIi-sz9g0x3LFvkIZjvK1r2ZaCqgGS/view?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgOGxPDP3Wpp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = open('./EVAAssignments/EVA2/Assignment3/text.txt', 'r').read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeXXMLRb4kXb",
        "colab_type": "text"
      },
      "source": [
        "Process data and calculate indices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5TKeiOp4jtl",
        "colab_type": "code",
        "outputId": "d58b3204-5bdd-4c4e-932c-54ce205686ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "chars = list(set(data))\n",
        "data_size, X_size = len(data), len(chars)\n",
        "print(\"Corona Virus article has %d characters, %d unique characters\" %(data_size, X_size))\n",
        "char_to_idx = {ch:i for i,ch in enumerate(chars)}\n",
        "idx_to_char = {i:ch for i,ch in enumerate(chars)}"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Corona Virus article has 10223 characters, 75 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4C53MB135LRY",
        "colab_type": "text"
      },
      "source": [
        "# Constants and Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfj21ORa49Ps",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Hidden_Layer_size = 100 #size of the hidden layer\n",
        "Time_steps = 40 # Number of time steps (length of the sequence) used for training\n",
        "learning_rate = 1e-1 # Learning Rate\n",
        "weight_sd = 0.1 #Standard deviation of weights for initialization\n",
        "z_size = Hidden_Layer_size + X_size #Size of concatenation(H, X) vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdmJf4Du5uhb",
        "colab_type": "text"
      },
      "source": [
        "# Activation Functions and Derivatives"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seGHei_D5FGk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(x): # sigmoid function\n",
        "    return 1/(1 + np.exp(-x))\n",
        "\n",
        "def dsigmoid(y): # derivative of sigmoid function\n",
        "    return y * (1-y)\n",
        "\n",
        "def tanh(x): # tanh function\n",
        "    return (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
        "\n",
        "def dtanh(y): # derivative of tanh\n",
        "    return 1-y**2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeCvVH1v6Me-",
        "colab_type": "text"
      },
      "source": [
        "# Quiz Question 1\n",
        "\n",
        "What is the value of sigmoid(0) calculated from  your code? (Answer up to 1 decimal point, e.g. 4.2 and NOT 4.29999999, no rounding off).\n",
        "\n",
        "# Quiz Question 2\n",
        "\n",
        "What is the value of dsigmoid(sigmoid(0)) calculated from your code?? (Answer up to 2 decimal point, e.g. 4.29 and NOT 4.29999999, no rounding off). \n",
        "\n",
        "# Quiz Question 3\n",
        "\n",
        "What is the value of tanh(dsigmoid(sigmoid(0))) calculated from your code?? (Answer up to 5 decimal point, e.g. 4.29999 and NOT 4.29999999, no rounding off).\n",
        "\n",
        "# Quiz Question 4\n",
        "\n",
        "What is the value of dtanh(tanh(dsigmoid(sigmoid(0)))) calculated from your code?? (Answer up to 5 decimal point, e.g. 4.29999 and NOT 4.29999999, no rounding off)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z25en6PuSGQa",
        "colab_type": "code",
        "outputId": "bf947316-a6ee-4a42-8d08-b6577897e9c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sigmoid(0)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5xbTDVISQBn",
        "colab_type": "code",
        "outputId": "97b30206-abfe-45bb-e33c-acdb66adedfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dsigmoid(sigmoid(0))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.25"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOY-nsAYSfeU",
        "colab_type": "code",
        "outputId": "ef6406c1-461d-4afa-cdc2-1e9a2b091f74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tanh(dsigmoid(sigmoid(0)))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.24491866240370908"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wdxut-JWSnLX",
        "colab_type": "code",
        "outputId": "f8a634c6-dbab-4539-c880-f27291452adc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dtanh(tanh(dsigmoid(sigmoid(0))))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.940014848806378"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeSVipDu8iKE",
        "colab_type": "text"
      },
      "source": [
        "# Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICbWNemE6LGV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Param:\n",
        "    def __init__(self, name, value):\n",
        "      self.name = name\n",
        "      self.v = value # parameter value\n",
        "      self.d = np.zeros_like(value) # derivative\n",
        "      self.m = np.zeros_like(value) # momentum for Adagrad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j83pZNPE8212",
        "colab_type": "text"
      },
      "source": [
        "We use random weights with normal distribution (0, weight_sd) for  tanh  activation function and (0.5, weight_sd) for  `sigmoid`  activation function.\n",
        "\n",
        "Biases are initialized to zeros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swHwLXOI9E7V",
        "colab_type": "text"
      },
      "source": [
        "# LSTM \n",
        "You are making this network, please note f, i, c and o (also \"v\") in the image below:\n",
        "![alt text](http://blog.varunajayasiri.com/ml/lstm.svg)\n",
        "\n",
        "Please note that we are concatenating the old_hidden_vector and new_input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0DBzNY-90s5",
        "colab_type": "text"
      },
      "source": [
        "# Quiz Question 4\n",
        "\n",
        "In the class definition below, what should be size_a, size_b, and size_c? ONLY use the variables defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFuHhqVq6Wge",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "size_a = Hidden_Layer_size\n",
        "size_b = z_size\n",
        "size_c = X_size\n",
        "\n",
        "class Parameters:\n",
        "    def __init__(self):\n",
        "        self.W_f = Param('W_f', np.random.randn(size_a, size_b) * weight_sd + 0.5)\n",
        "        self.b_f = Param('b_f', np.zeros((size_a, 1)))\n",
        "\n",
        "        self.W_i = Param('W_i', np.random.randn(size_a, size_b) * weight_sd + 0.5)\n",
        "        self.b_i = Param('b_i', np.zeros((size_a, 1)))\n",
        "\n",
        "        self.W_C = Param('W_C', np.random.randn(size_a, size_b) * weight_sd)\n",
        "        self.b_C = Param('b_C', np.zeros((size_a, 1)))\n",
        "\n",
        "        self.W_o = Param('W_o', np.random.randn(size_a, size_b) * weight_sd + 0.5)\n",
        "        self.b_o = Param('b_o', np.zeros((size_a, 1)))\n",
        "\n",
        "        #For final layer to predict the next character\n",
        "        self.W_v = Param('W_v', np.random.randn(X_size, size_a) * weight_sd)\n",
        "        self.b_v = Param('b_v', np.zeros((size_c, 1)))\n",
        "        \n",
        "    def all(self):\n",
        "        return [self.W_f, self.W_i, self.W_C, self.W_o, self.W_v, self.b_f, self.b_i, self.b_C, self.b_o, self.b_v]\n",
        "        \n",
        "parameters = Parameters()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzmfGLZt_xVs",
        "colab_type": "text"
      },
      "source": [
        "Look at these operations which we'll be writing:\n",
        "\n",
        "**Concatenation of h and x:**\n",
        "\n",
        "$z\\:=\\:\\left[h_{t-1},\\:x\\right]$\n",
        "\n",
        "$f_t=\\sigma\\left(W_f\\cdot z\\:+\\:b_f\\:\\right)$\n",
        "\n",
        "$i_i=\\sigma\\left(W_i\\cdot z\\:+\\:b_i\\right)$\n",
        "\n",
        "$\\overline{C_t}=\\tanh\\left(W_C\\cdot z\\:+\\:b_C\\right)$\n",
        "\n",
        "$C_t=f_t\\ast C_{t-1}+i_t\\ast \\overline{C}_t$\n",
        "\n",
        "$o_t=\\sigma\\left(W_o\\cdot z\\:+\\:b_i\\right)$\n",
        "\n",
        "$h_t=o_t\\ast\\tanh\\left(C_t\\right)$\n",
        "\n",
        "**Logits:**\n",
        "\n",
        "$v_t=W_v\\cdot h_t+b_v$\n",
        "\n",
        "**Softmax:**\n",
        "\n",
        "$\\hat{y}=softmax\\left(v_t\\right)$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bUkseNnDott",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward(x, h_prev, C_prev, p = parameters):\n",
        "    assert x.shape == (X_size, 1)\n",
        "    assert h_prev.shape == (Hidden_Layer_size, 1)\n",
        "    assert C_prev.shape == (Hidden_Layer_size, 1)\n",
        "    \n",
        "    z = np.row_stack((h_prev, x))\n",
        "    f = sigmoid( np.dot(p.W_f.v, z) + p.b_f.v)\n",
        "    i = sigmoid( np.dot(p.W_i.v, z) + p.b_i.v)\n",
        "    C_bar = tanh( np.dot(p.W_C.v, z)+ p.b_C.v)\n",
        "\n",
        "    C = f * C_prev + i * C_bar\n",
        "    o = sigmoid( np.dot(p.W_o.v, z) + p.b_i.v)\n",
        "    h = o * tanh(C)\n",
        "\n",
        "    v = np.dot(p.W_v.v, h) + p.b_v.v\n",
        "    y = np.exp(v) / np.sum(np.exp(v)) #softmax\n",
        "\n",
        "    return z, f, i, C_bar, C, o, h, v, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZrDhZIjFpdI",
        "colab_type": "text"
      },
      "source": [
        "You must finish the function above before you can attempt the questions below. \n",
        "\n",
        "# Quiz Question 5\n",
        "\n",
        "What is the output of 'print(len(forward(np.zeros((X_size, 1)), np.zeros((Hidden_Layer_size, 1)), np.zeros((Hidden_Layer_size, 1)), parameters)))'?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WG--0rjFoYrg",
        "colab_type": "code",
        "outputId": "ebc6a532-9c12-4761-ef46-8f6f24743e9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(forward(np.zeros((X_size, 1)), np.zeros((Hidden_Layer_size, 1)), np.zeros((Hidden_Layer_size, 1)), parameters)))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XV-YVl_GGiX8",
        "colab_type": "text"
      },
      "source": [
        "# Quiz Question 6. \n",
        "\n",
        "Assuming you have fixed the forward function, run this command: \n",
        "z, f, i, C_bar, C, o, h, v, y = forward(np.zeros((X_size, 1)), np.zeros((Hidden_Layer_size, 1)), np.zeros((Hidden_Layer_size, 1)))\n",
        "\n",
        "Now, find these values:\n",
        "\n",
        "\n",
        "1.   print(z.shape)\n",
        "2.   print(np.sum(z))\n",
        "3.   print(np.sum(f))\n",
        "\n",
        "Copy and paste exact values you get in the logs into the quiz.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GvKVWmTDt3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "z, f, i, C_bar, C, o, h, v, y = forward(np.zeros((X_size, 1)), np.zeros((Hidden_Layer_size, 1)), np.zeros((Hidden_Layer_size, 1)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHqvt_Lo3orv",
        "colab_type": "code",
        "outputId": "671057de-4003-487e-b8f8-9af46c88b860",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "print(z.shape)\n",
        "print(np.sum(z))\n",
        "print(np.sum(f))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(175, 1)\n",
            "0.0\n",
            "50.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeSvhkqwILsG",
        "colab_type": "text"
      },
      "source": [
        "# Backpropagation\n",
        "\n",
        "Here we are defining the backpropagation. It's too complicated, here is the whole code. (Please note that this would work only if your earlier code is perfect)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIa1jUZiGPmF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backward(target, dh_next, dC_next, C_prev,\n",
        "             z, f, i, C_bar, C, o, h, v, y,\n",
        "             p = parameters):\n",
        "    \n",
        "    assert z.shape == (X_size + Hidden_Layer_size, 1)\n",
        "    assert v.shape == (X_size, 1)\n",
        "    assert y.shape == (X_size, 1)\n",
        "    \n",
        "    for param in [dh_next, dC_next, C_prev, f, i, C_bar, C, o, h]:\n",
        "        assert param.shape == (Hidden_Layer_size, 1)\n",
        "        \n",
        "    dv = np.copy(y)\n",
        "    dv[target] -= 1\n",
        "\n",
        "    p.W_v.d += np.dot(dv, h.T)\n",
        "    p.b_v.d += dv\n",
        "\n",
        "    dh = np.dot(p.W_v.v.T, dv)        \n",
        "    dh += dh_next\n",
        "    do = dh * tanh(C)\n",
        "    do = dsigmoid(o) * do\n",
        "    p.W_o.d += np.dot(do, z.T)\n",
        "    p.b_o.d += do\n",
        "\n",
        "    dC = np.copy(dC_next)\n",
        "    dC += dh * o * dtanh(tanh(C))\n",
        "    dC_bar = dC * i\n",
        "    dC_bar = dtanh(C_bar) * dC_bar\n",
        "    p.W_C.d += np.dot(dC_bar, z.T)\n",
        "    p.b_C.d += dC_bar\n",
        "\n",
        "    di = dC * C_bar\n",
        "    di = dsigmoid(i) * di\n",
        "    p.W_i.d += np.dot(di, z.T)\n",
        "    p.b_i.d += di\n",
        "\n",
        "    df = dC * C_prev\n",
        "    df = dsigmoid(f) * df\n",
        "    p.W_f.d += np.dot(df, z.T)\n",
        "    p.b_f.d += df\n",
        "\n",
        "    dz = (np.dot(p.W_f.v.T, df)\n",
        "         + np.dot(p.W_i.v.T, di)\n",
        "         + np.dot(p.W_C.v.T, dC_bar)\n",
        "         + np.dot(p.W_o.v.T, do))\n",
        "    dh_prev = dz[:Hidden_Layer_size, :]\n",
        "    dC_prev = f * dC\n",
        "    \n",
        "    return dh_prev, dC_prev"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tnc7WpRkIU5S",
        "colab_type": "text"
      },
      "source": [
        "# Forward and Backward Combined Pass\n",
        "\n",
        "Let's first clear the gradients before each backward pass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJWoC3U1ITf8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clear_gradients(params = parameters):\n",
        "    for p in params.all():\n",
        "        p.d.fill(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XN93UnjIgmA",
        "colab_type": "text"
      },
      "source": [
        "Clip gradients to mitigate exploding gradients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LTsublxIfFl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clip_gradients(params = parameters):\n",
        "    for p in params.all():\n",
        "        np.clip(p.d, -1, 1, out=p.d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7XUpDTWIl_Y",
        "colab_type": "text"
      },
      "source": [
        "Calculate and store the values in forward pass. Accumulate gradients in backward pass and clip gradients to avoid exploding gradients.\n",
        "\n",
        "input, target are list of integers, with character indexes.\n",
        "h_prev is the array of initial h at  h−1  (size H x 1)\n",
        "C_prev is the array of initial C at  C−1  (size H x 1)\n",
        "Returns loss, final  hT  and  CT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQNxjTuZIia_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_backward(inputs, targets, h_prev, C_prev):\n",
        "    global paramters\n",
        "    \n",
        "    # To store the values for each time step\n",
        "    x_s, z_s, f_s, i_s,  = {}, {}, {}, {}\n",
        "    C_bar_s, C_s, o_s, h_s = {}, {}, {}, {}\n",
        "    v_s, y_s =  {}, {}\n",
        "    \n",
        "    # Values at t - 1\n",
        "    h_s[-1] = np.copy(h_prev)\n",
        "    C_s[-1] = np.copy(C_prev)\n",
        "    \n",
        "    loss = 0\n",
        "    # Loop through time steps\n",
        "    assert len(inputs) == Time_steps\n",
        "    for t in range(len(inputs)):\n",
        "        x_s[t] = np.zeros((X_size, 1))\n",
        "        x_s[t][inputs[t]] = 1 # Input character\n",
        "        \n",
        "        (z_s[t], f_s[t], i_s[t],\n",
        "        C_bar_s[t], C_s[t], o_s[t], h_s[t],\n",
        "        v_s[t], y_s[t]) = \\\n",
        "            forward(x_s[t], h_s[t - 1], C_s[t - 1]) # Forward pass\n",
        "            \n",
        "        loss += -np.log(y_s[t][targets[t], 0]) # Loss for at t\n",
        "        \n",
        "    clear_gradients()\n",
        "\n",
        "    dh_next = np.zeros_like(h_s[0]) #dh from the next character\n",
        "    dC_next = np.zeros_like(C_s[0]) #dh from the next character\n",
        "\n",
        "    for t in reversed(range(len(inputs))):\n",
        "        # Backward pass\n",
        "        dh_next, dC_next = \\\n",
        "            backward(target = targets[t], dh_next = dh_next,\n",
        "                     dC_next = dC_next, C_prev = C_s[t-1],\n",
        "                     z = z_s[t], f = f_s[t], i = i_s[t], C_bar = C_bar_s[t],\n",
        "                     C = C_s[t], o = o_s[t], h = h_s[t], v = v_s[t],\n",
        "                     y = y_s[t])\n",
        "\n",
        "    clip_gradients()\n",
        "        \n",
        "    return loss, h_s[len(inputs) - 1], C_s[len(inputs) - 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcy5u_vRItkV",
        "colab_type": "text"
      },
      "source": [
        "# Sample the next character"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8SrtJiwIsSm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(h_prev, C_prev, first_char_idx, sentence_length):\n",
        "    x = np.zeros((X_size, 1))\n",
        "    x[first_char_idx] = 1\n",
        "\n",
        "    h = h_prev\n",
        "    C = C_prev\n",
        "\n",
        "    indexes = []\n",
        "    \n",
        "    for t in range(sentence_length):\n",
        "        _, _, _, _, C, _, h, _, p = forward(x, h, C)\n",
        "        idx = np.random.choice(range(X_size), p=p.ravel())\n",
        "        x = np.zeros((X_size, 1))\n",
        "        x[idx] = 1\n",
        "        indexes.append(idx)\n",
        "\n",
        "    return indexes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiWFaWLNIx_L",
        "colab_type": "text"
      },
      "source": [
        "# Training (Adagrad)\n",
        "\n",
        "Update the graph and display a sample output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENQYU-7AIw0t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_status(inputs, h_prev, C_prev):\n",
        "    #initialized later\n",
        "    global plot_iter, plot_loss\n",
        "    global smooth_loss\n",
        "    \n",
        "    # Get predictions for 200 letters with current model\n",
        "\n",
        "    sample_idx = sample(h_prev, C_prev, inputs[0], 200)\n",
        "    txt = ''.join(idx_to_char[idx] for idx in sample_idx)\n",
        "\n",
        "    # Clear and plot\n",
        "    plt.plot(plot_iter, plot_loss)\n",
        "    display.clear_output(wait=True)\n",
        "    plt.show()\n",
        "\n",
        "    #Print prediction and loss\n",
        "    print(\"----\\n %s \\n----\" % (txt, ))\n",
        "    print(\"iter %d, loss %f\" % (iteration, smooth_loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACXcASJuI73a",
        "colab_type": "text"
      },
      "source": [
        "# Update Parameters\n",
        "\n",
        "\\begin{align}\n",
        "\\theta_i &= \\theta_i - \\eta\\frac{d\\theta_i}{\\sum dw_{\\tau}^2} \\\\\n",
        "d\\theta_i &= \\frac{\\partial L}{\\partial \\theta_i}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bR08TvcjI4Pf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_paramters(params = parameters):\n",
        "    for p in params.all():\n",
        "        p.m += p.d * p.d # Calculate sum of gradients\n",
        "        #print(learning_rate * dparam)\n",
        "        p.v += -(learning_rate * p.d / np.sqrt(p.m + 1e-8))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "La9vyJ6RJLFK",
        "colab_type": "text"
      },
      "source": [
        "To delay the keyboard interrupt to prevent the training from stopping in the middle of an iteration\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVDHbMb7JNGT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Exponential average of loss\n",
        "# Initialize to a error of a random model\n",
        "smooth_loss = -np.log(1.0 / X_size) * Time_steps\n",
        "\n",
        "iteration, pointer = 0, 0\n",
        "\n",
        "# For the graph\n",
        "plot_iter = np.zeros((0))\n",
        "plot_loss = np.zeros((0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HF6vS0VWJqsS",
        "colab_type": "text"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQyNSL0iJOxH",
        "colab_type": "code",
        "outputId": "0b71b495-ff5a-4def-861b-8c20bd46ff56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        }
      },
      "source": [
        "iter = 50000\n",
        "while iter > 0:\n",
        "  # Reset\n",
        "  if pointer + Time_steps >= len(data) or iteration == 0:\n",
        "      g_h_prev = np.zeros((Hidden_Layer_size, 1))\n",
        "      g_C_prev = np.zeros((Hidden_Layer_size, 1))\n",
        "      pointer = 0\n",
        "\n",
        "\n",
        "  inputs = ([char_to_idx[ch] \n",
        "              for ch in data[pointer: pointer + Time_steps]])\n",
        "  targets = ([char_to_idx[ch] \n",
        "              for ch in data[pointer + 1: pointer + Time_steps + 1]])\n",
        "\n",
        "  loss, g_h_prev, g_C_prev = \\\n",
        "      forward_backward(inputs, targets, g_h_prev, g_C_prev)\n",
        "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
        "\n",
        "  # Print every hundred steps\n",
        "  if iteration % 100 == 0:\n",
        "      update_status(inputs, g_h_prev, g_C_prev)\n",
        "\n",
        "  update_paramters()\n",
        "\n",
        "  plot_iter = np.append(plot_iter, [iteration])\n",
        "  plot_loss = np.append(plot_loss, [loss])\n",
        "\n",
        "  pointer += Time_steps\n",
        "  iteration += 1\n",
        "  iter = iter -1"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAAD1CAYAAACm0cXeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVT2IkTAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "----\n",
            " cy of drangh Air 2003. con'scon includ can spread?\n",
            "Hot an treaten some the fursentolding an the screening lan to tuest.\n",
            "\n",
            "Aronavelueet for dost ras iltaves and of cases, or with a health Orat sey out f \n",
            "----\n",
            "iter 49900, loss 7.224629\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Gx4ODLUUx9P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}